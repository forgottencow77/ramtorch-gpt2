data:
  dataset_name: hotchpotch/fineweb-2-edu-japanese
  dataset_config: sample_10BT
  dataset_path: null
  text_column: text
  train_split: train
  eval_split: test
  max_train_samples: 1000000
  max_eval_samples: 10000
  seq_len: 768
  micro_batch_size: 2
  gradient_accumulation: 64
  num_workers: 2
  streaming: true
  buffer_size: 8192
  tokenizer_name: gpt2
  tokenizer_vocab_size: 50257
  tokenizer_pad_to_multiple_of: 128
  pack_sequences: true

model:
  vocab_size: 50257
  n_layer: 18
  n_head: 12
  n_embd: 768
  max_position_embeddings: 768
  rotary_pct: null
  activation_fn: gelu
  resid_pdrop: 0.02
  embd_pdrop: 0.02
  attn_pdrop: 0.02
  layer_norm_epsilon: 1.0e-5
  use_flash_attn: true
  gradient_checkpointing: true
  ram_offload_device: cuda
  ram_cpu_buffer: 2
  dtype: float16

optim:
  learning_rate: 0.0004
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8
  grad_clip: 1.0

scheduler:
  warmup_steps: 2000
  max_steps: 200000
  min_lr_ratio: 0.1

checkpoint:
  output_dir: checkpoints_small
  save_interval_steps: 1000
  keep_last_n: 5
  resume_path: null
  hf_repo_id: null
  hf_token: null
  push_to_hub_interval: 5000

logging:
  log_interval: 20
  eval_interval: 2000
  use_wandb: true
  wandb_project: ramgpt-gpt2-0p1b
  wandb_run_name: null
  wandb_watch: false

seed: 1337
device: cuda
compile: false
bf16: false
fp16: true
